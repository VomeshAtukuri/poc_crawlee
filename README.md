

# ğŸ•·ï¸ AI-Powered Web Crawler & Q&A System

A full-stack application that crawls websites, stores extracted content in a vector database (**LanceDB**), and allows users to ask questions with context-aware, AI-generated answers.

## ğŸ“Œ Overview

This project is a combination of **web scraping**, **vector search**, and **AI-driven Q&A**, built using modern web technologies.

> **Goal:** Allow users to crawl any public website, extract meaningful content, store it semantically, and interact with that content via natural language queries.

## ğŸ§± Architecture

**Frontend**:  
Built with **React + Vite**, the UI lets users input a URL for crawling and ask questions.  

**Backend**:  
Runs on **Fastify**, handles crawling via **PlaywrightCrawler**, processes content with **embedding models**, stores data in **LanceDB**, and returns intelligent answers using a vector search + LLM combo.

```bash
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚   Frontend  â”‚
             â”‚ React + Viteâ”‚
             â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚    Fastify API   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   /crawl /chat   â”‚             â”‚
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                 â–¼                         â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
     â”‚   PlaywrightCrawler   â”‚             â”‚
     â”‚   Extract Text Data   â”‚             â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                â–¼                          â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
     â”‚     Embedding Model        â”‚        â”‚
     â”‚  (e.g. OpenAI / Groq etc.) â”‚        â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
                â–¼                          â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
         â”‚   LanceDB (VDB)    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
## âœ¨ Features

âœ… Crawl any public website using headless browser  
âœ… Extract meaningful page content (skips scripts, styles, boilerplate)  
âœ… Embed data into vectors using state-of-the-art LLMs  
âœ… Store and query with **LanceDB**  
âœ… Ask natural language questions and get contextually relevant answers  
âœ… Built on a **scalable**, **modular**, and **modern** full-stack setup  

## ğŸ› ï¸ Tech Stack

| Layer        | Tech                      |
|--------------|---------------------------|
| Frontend     | React, Vite, TypeScript   |
| Backend      | Fastify, Node.js, TypeScript |
| Crawler      | Crawlee (PlaywrightCrawler) |
| Vector Store | LanceDB                   |
| Embedding    | OpenAI / Groq             |


## âš™ï¸ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/VomeshAtukuri/poc_crawlee.git
cd poc_crawlee
```

### 2. Install Dependencies

**Backend**

```bash
cd backend
npm install
```

**Frontend**

```bash
cd ../frontend
npm install
```

### 3. Environment Setup

In `backend/`, create a `.env` file:

```env
GROQ_API_KEY = your_groq_or_openai_key
```

### 4. Start Development Servers

**Start Backend**

```bash
cd backend
npm start
```

**Start Frontend**

```bash
cd ../frontend
npm run dev
```


## ğŸ’¡ Usage Guide

1. Enter a URL in the frontend.
2. Backend crawls and processes the content.
3. Content is stored as vector embeddings in LanceDB.
4. Ask a question based on the crawled site.
5. AI generates a context-aware answer using vector similarity search.


## ğŸ§  Example Use Cases

* ğŸ” Competitor website analysis
* ğŸ“š Knowledge base ingestion & querying
* ğŸ“° News site summarization and Q\&A
* ğŸ› ï¸ Internal tool for crawling documentation sites


## ğŸš§ Future Enhancements

* ğŸ›¡ï¸ User authentication and usage limit
* ğŸŒ Crawl multi-page or entire domains
* ğŸ§© Add summarization and topic tagging
* ğŸ“Š Visualize crawl data insights
* âš™ï¸ Docker + CI/CD support

